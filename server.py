import os
import io
import json
import glob
import traceback
import time
import base64
from io import BytesIO

from dotenv import load_dotenv
from flask import Flask, request, jsonify, render_template, send_file
from flask_cors import CORS
import pandas as pd
import PIL.Image
import re
import html
import difflib  # ğŸ”¹ OCR ì˜ì‹¬ íŒë³„ìš©

from openai import OpenAI

# Optional OCR fallback libraries
try:
    import pytesseract
    TESSERACT_AVAILABLE = True
except Exception:
    TESSERACT_AVAILABLE = False

try:
    from pdf2image import convert_from_bytes
    PDF2IMAGE_AVAILABLE = True
except Exception:
    PDF2IMAGE_AVAILABLE = False


# =======================
#  ê¸°ë³¸ ì„¤ì •
# =======================

load_dotenv()
app = Flask(__name__)
app.config["JSON_AS_ASCII"] = False
CORS(app)

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    print("ğŸš¨ ê²½ê³ : .env íŒŒì¼ì— OPENAI_API_KEYê°€ ì—†ìŠµë‹ˆë‹¤!")

client = OpenAI(api_key=OPENAI_API_KEY) if OPENAI_API_KEY else OpenAI()

TEXT_MODEL = os.getenv("OPENAI_TEXT_MODEL", "gpt-5.1-mini")
VISION_MODEL = os.getenv("OPENAI_VISION_MODEL", "gpt-4.1-mini")


# =======================
#  OpenAI ìœ í‹¸ í•¨ìˆ˜
# =======================

def call_openai_response(model: str, input_data, *, response_format=None, max_retries: int = 3):
    """
    OpenAI Responses API í˜¸ì¶œ + ê°„ë‹¨ Retry
    """
    last_err = None
    for attempt in range(1, max_retries + 1):
        try:
            kwargs = {
                "model": model,
                "input": input_data,
            }
            if response_format:
                kwargs["response_format"] = response_format

            resp = client.responses.create(**kwargs)
            return resp
        except Exception as e:
            last_err = e
            print(f"âš ï¸ OpenAI í˜¸ì¶œ ì‹¤íŒ¨ {attempt}/{max_retries}: {e}")
            if attempt < max_retries:
                time.sleep(2 * attempt)
    raise last_err


def extract_output_text_from_response(response) -> str:
    """
    OpenAI Responses API ì‘ë‹µì—ì„œ text ë¶€ë¶„ë§Œ ì¶”ì¶œ
    """
    try:
        output_items = getattr(response, "output", None)
        if output_items:
            texts = []
            for item in output_items:
                contents = getattr(item, "content", None) or []
                for c in contents:
                    if getattr(c, "type", None) == "output_text":
                        texts.append(getattr(c, "text", ""))
            if texts:
                return "\n".join(texts).strip()
    except Exception as e:
        print(f"âš ï¸ ì‘ë‹µ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘ ì˜ˆì™¸: {e}")

    if isinstance(response, dict):
        output_items = response.get("output", [])
        if output_items:
            contents = output_items[0].get("content", [])
            if contents and contents[0].get("type") == "output_text":
                return contents[0].get("text", "")

    return str(response)


def resize_image_bytes(image_bytes: bytes, max_size: int = 1500) -> tuple[bytes, str]:
    """
    ë©”ëª¨ë¦¬ ì ˆì•½ + OCR ì„±ëŠ¥ ìœ ì§€ìš© ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì¦ˆ
    - ê¸´ ë³€ì´ max_sizeë¥¼ ë„˜ìœ¼ë©´ ë¹„ìœ¨ ìœ ì§€í•˜ë©° ë¦¬ì‚¬ì´ì¦ˆ
    - JPEG(ë˜ëŠ” ì›ë³¸ í¬ë§·)ë¡œ ì¬ì €ì¥ (quality=85ë¡œ ê°€ë³ê²Œ)
    """
    img = PIL.Image.open(io.BytesIO(image_bytes))

    if max(img.size) > max_size:
        ratio = max_size / max(img.size)
        new_size = (int(img.width * ratio), int(img.height * ratio))
        img = img.resize(new_size, PIL.Image.Resampling.LANCZOS)
        print(f"ğŸ“‰ ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì§•: {img.size}")
    else:
        print(f"âœ… ë¦¬ì‚¬ì´ì§• ë¶ˆí•„ìš”: {img.size}")

    fmt = img.format if img.format else "JPEG"
    buf = io.BytesIO()
    img.save(buf, format=fmt, quality=85)
    buf.seek(0)
    return buf.read(), fmt


def combine_parts_to_prompt(parts) -> str:
    """
    parts ë¦¬ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ë¡œ í•©ì¹˜ê¸°
    - ë¬¸ìì—´: ê·¸ëŒ€ë¡œ
    - {"text": "..."}: text í•„ë“œ ì‚¬ìš©
    """
    chunks = []
    for p in parts:
        if isinstance(p, str):
            chunks.append(p)
        elif isinstance(p, dict) and "text" in p:
            chunks.append(str(p["text"]))
    return "\n\n".join(chunks)


# =======================
#  ë²•ë ¹ í…ìŠ¤íŠ¸ ë¡œë“œ
# =======================

def load_law_texts() -> str:
    print("ğŸ“š ë²•ë ¹ íŒŒì¼ë“¤ì„ ì½ì–´ì˜¤ëŠ” ì¤‘...")
    law_files = glob.glob("law_text_*.txt") + glob.glob("../law_text_*.txt")
    if not law_files:
        print("âš ï¸ ë²•ë ¹ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ë²•ë¥  ê²€í†  ê¸°ëŠ¥ì´ ì œí•œë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        return ""
    all_law_text = ""
    for file_path in law_files:
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                all_law_text += f"--- ë²•ë ¹ [{file_path}] ì‹œì‘ ---\n\n"
                all_law_text += f.read()
                all_law_text += f"\n\n--- ë²•ë ¹ [{file_path}] ë ---\n\n"
            print(f"âœ… ë²•ë ¹ íŒŒì¼ '{file_path}' ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            print(f"âŒ ë²•ë ¹ íŒŒì¼ '{file_path}' ì½ê¸° ì‹¤íŒ¨: {e}")
    print(f"âœ… ëª¨ë“  ë²•ë ¹ íŒŒì¼ ë¡œë“œ ì™„ë£Œ (ì´ {len(all_law_text)}ì)")
    return all_law_text


ALL_LAW_TEXT = load_law_texts()


# =======================
#  í”„ë¡¬í”„íŠ¸ë“¤
# =======================

PROMPT_EXTRACT_INGREDIENT_INFO = """
ì´ ì´ë¯¸ì§€ëŠ” ì›ë¶€ì¬ë£Œ í‘œì‹œì‚¬í•­ ì‚¬ì§„ì…ë‹ˆë‹¤. 
ì´ ì´ë¯¸ì§€ëŠ” ì›ë¶€ì¬ë£Œ í‘œì‹œì‚¬í•­ ì‚¬ì§„ì…ë‹ˆë‹¤.   
**í•„ìˆ˜ì ìœ¼ë¡œ ì¶”ì¶œí•´ì•¼ í•  ì •ë³´ë§Œ** ì¶”ì¶œí•˜ì„¸ìš”.

[ì¶”ì¶œí•´ì•¼ í•  ì •ë³´]
1. **ì›ì¬ë£Œëª…**: ì›ì¬ë£Œì˜ ì •í™•í•œ ëª…ì¹­
2. **ë³µí•©ì›ì¬ë£Œ ë‚´ì—­**: ê´„í˜¸ ì•ˆì˜ í•˜ìœ„ ì›ì¬ë£Œ ì •ë³´ (ì˜ˆ: (íƒˆì§€ëŒ€ë‘, ì†Œë§¥))
3. **ì›ì‚°ì§€ ì •ë³´**: ì›ì‚°ì§€ í‘œê¸° (ì˜ˆ: ì™¸êµ­ì‚°, êµ­ë‚´ì‚°, ì¸ë„ì‚° ë“±)
4. **ì•Œë ˆë¥´ê¸° ìœ ë°œë¬¼ì§ˆ**: ì•Œë ˆë¥´ê¸° í‘œì‹œ ì •ë³´
5. **ì‹í’ˆì²¨ê°€ë¬¼**: ì²¨ê°€ë¬¼ëª…ê³¼ ìš©ë„ ë³‘ê¸° ì—¬ë¶€

[ì¶”ì¶œí•˜ì§€ ë§ì•„ì•¼ í•  ì •ë³´]
- ë³´ê´€ë°©ë²• (ì˜ˆ: ëƒ‰ì¥ë³´ê´€, ì‹¤ì˜¨ë³´ê´€ ë“±)
- í¬ì¥ì¬ì§ˆ ì •ë³´
- ë¶„ë¦¬ë°°ì¶œ ë§ˆí¬
- ë°”ì½”ë“œ ë²ˆí˜¸
- ì œì¡°ì¼ì/ìœ í†µê¸°í•œ
- ë‹¨ìˆœ í™ë³´ ë¬¸êµ¬
- ê¸°íƒ€ í‘œì‹œì‚¬í•­ê³¼ ë¬´ê´€í•œ ì •ë³´

[ì¶œë ¥ í˜•ì‹]
JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”:
{
    "ingredient_name": "ì›ì¬ë£Œëª…",
    "sub_ingredients": "í•˜ìœ„ì›ì¬ë£Œ ë‚´ì—­ (ë³µí•©ì›ì¬ë£Œì¸ ê²½ìš°)",
    "origin": "ì›ì‚°ì§€ ì •ë³´",
    "allergens": ["ì•Œë ˆë¥´ê¸° ìœ ë°œë¬¼ì§ˆ ëª©ë¡"],
    "additives": ["ì‹í’ˆì²¨ê°€ë¬¼ ëª©ë¡"]
}

ì›ì¬ë£Œëª…ì´ ëª…í™•í•˜ì§€ ì•Šìœ¼ë©´ "ingredient_name"ì„ ë¹ˆ ë¬¸ìì—´ë¡œ ë‘ì„¸ìš”.
"""

PROMPT_VERIFY_DESIGN = """
ë‹¹ì‹ ì€ ì‹í’ˆí‘œì‹œì‚¬í•­ ê°ì‚¬ê´€ì´ì ë²•ë¥  ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
[ê¸°ì¤€ ë°ì´í„°(Standard)]ì™€ [ë””ìì¸ ì‹œì•ˆ(Design)]ì„ ë¹„êµí•˜ì—¬ ì˜¤ë¥˜ë¥¼ ê²€ì¶œí•˜ì„¸ìš”.

[ì…ë ¥]
1. **Standard**: ì•ì„œ ìƒì„±ëœ ì™„ë²½í•œ í‘œì‹œì‚¬í•­ ì •ë‹µì§€
2. **Design OCR í…ìŠ¤íŠ¸**: ì‹¤ì œ í¬ì¥ì§€ ë””ìì¸ íŒŒì¼ì—ì„œ OCRë¡œ ì¶”ì¶œí•œ ìˆœìˆ˜ í…ìŠ¤íŠ¸
3. **ë²•ë ¹**: ì‹í’ˆ í‘œì‹œ ê´€ë ¨ ë²•ë ¹

[ê²€ì¦ ì›ì¹™ - ë§¤ìš° ì¤‘ìš”! ë°˜ë“œì‹œ ì¤€ìˆ˜í•˜ì„¸ìš”!]
1. **ì˜¤íƒˆì ê²€ì¶œ ì¤‘ì‹¬**: Standardì™€ Designì„ ë¬¸ì ë‹¨ìœ„ë¡œ ì •í™•íˆ ë¹„êµí•˜ì—¬ ì‹¤ì œ ì˜¤íƒˆìë§Œ ê²€ì¶œí•˜ì„¸ìš”.
2. **í•¨ëŸ‰ ì •ë³´(%) ì¶”ê°€ëŠ” í—ˆìš©**: Standardì— ì—†ì–´ë„ Designì— í•¨ëŸ‰ ì •ë³´(%)ê°€ ì¶”ê°€ëœ ê²ƒì€ ì ˆëŒ€ ë¬¸ì œë¡œ ë³´ì§€ ì•ŠìŠµë‹ˆë‹¤.
   âœ… í—ˆìš© ì˜ˆì‹œ: Standard "ë‹¹ê·¼(êµ­ë‚´ì‚°)" â†’ Design "ë‹¹ê·¼(êµ­ë‚´ì‚°) 4.1%" (ë¬¸ì œ ì—†ìŒ)
   âœ… í—ˆìš© ì˜ˆì‹œ: Standard "ì–‘íŒŒ" â†’ Design "ì–‘íŒŒ 2.2%" (ë¬¸ì œ ì—†ìŒ)
3. **ë¹„ì •ìƒ ê°’ë§Œ ë¬¸ì œ**: í•¨ëŸ‰ì´ 100%ë¥¼ ì´ˆê³¼í•˜ê±°ë‚˜ ë§ë„ ì•ˆë˜ëŠ” ê°’ì¸ ê²½ìš°ë§Œ ë¬¸ì œë¡œ í‘œì‹œí•©ë‹ˆë‹¤.
   âŒ ë¬¸ì œ ì˜ˆì‹œ: "ì–‘íŒŒ221%" (ì†Œìˆ˜ì  ëˆ„ë½ìœ¼ë¡œ 221%ê°€ ë˜ì–´ ë¹„ì •ìƒ) â†’ "ì–‘íŒŒ2.21%"ë¡œ ìˆ˜ì • í•„ìš”
   âŒ ë¬¸ì œ ì˜ˆì‹œ: "ë‹¹ê·¼999%" (ë§ë„ ì•ˆë˜ëŠ” ê°’) â†’ ë¬¸ì œë¡œ í‘œì‹œ
4. **ë¼ë²¨ëª… ëˆ„ë½ì€ ë¬´ì‹œ**: ë‚´ìš©ì´ë‚˜ ìˆ˜ì¹˜ëŠ” ìˆì§€ë§Œ ë¼ë²¨ëª…(ì˜ˆ: "ì „ë©´ë¶€ ì´ì—´ëŸ‰", "ì œì¡°ì‹œì„¤ì•ˆë‚´")ë§Œ ì—†ëŠ” ê²½ìš°ëŠ” ë¬¸ì œë¡œ ë³´ì§€ ì•ŠìŠµë‹ˆë‹¤.
   âœ… í—ˆìš©: ì˜ì–‘ì •ë³´ì— 127Kcal ìˆ˜ì¹˜ê°€ ìˆìœ¼ë©´ "ì „ë©´ë¶€ ì´ì—´ëŸ‰" ë¼ë²¨ì´ ì—†ì–´ë„ ë¬¸ì œ ì—†ìŒ
   âœ… í—ˆìš©: ì œì¡°ì‹œì„¤ ë‚´ìš©ì´ ìˆìœ¼ë©´ "ì œì¡°ì‹œì„¤ì•ˆë‚´" ë¼ë²¨ì´ ì—†ì–´ë„ ë¬¸ì œ ì—†ìŒ
5. **ì‹¤ì œ ì˜¤ë¥˜ë§Œ ê²€ì¶œ**:
   âœ… ì›ì¬ë£Œëª… ì˜¤íƒˆì: "ì „ë¶„ê°€ê³µí’ˆ" â†’ "ì „ë°˜ê°€ê³µí’ˆ" (ê¸€ì ì˜¤ê¸°)
   âœ… ì›ì¬ë£Œëª… ì˜¤íƒˆì: "D-ì†Œë¹„í†¨" â†’ "D-ì†”ë¹„í†¨" (ê¸€ì ì˜¤ê¸°)
   âœ… ìˆ«ì ì˜¤íƒˆì: "130kcal" â†’ "127kcal" (ìˆ«ì ì˜¤ê¸°)
   âœ… ë‹¨ìœ„ ì˜¤íƒˆì: "10g" â†’ "10mg" (ë‹¨ìœ„ ì˜¤ê¸°)
   âœ… êµ¬ë‘ì  ì˜¤íƒˆì: "ìš°ìœ , ì‡ ê³ ê¸°, í† ë§ˆí† " â†’ "ìš°ìœ  ì‡ ê³ ê¸° í† ë§ˆí† " (ì‰¼í‘œ ëˆ„ë½)
   âœ… ì†Œìˆ˜ì  ëˆ„ë½: "2.21%" â†’ "221%" (ë¹„ì •ìƒ ê°’)
   âœ… ì›ì‚°ì§€ ì˜¤ê¸°: "êµ­ë‚´ì‚°" â†’ "ìˆ˜ì…ì‚°" (ë‚´ìš© ì˜¤ê¸°)
   âœ… ìˆœì„œ ìœ„ë°˜: ë°°í•©ë¹„ ìˆœì„œì™€ ë‹¤ë¦„
   âœ… ë²•ë¥  ìœ„ë°˜: ì²¨ê°€ë¬¼ ìœ í˜• ëˆ„ë½ (ì˜ˆ: "ì†Œë¸Œì‚°ì¹¼ë¥¨" â†’ "ì†Œë¸Œì‚°ì¹¼ë¥¨(ë³´ì¡´ë£Œ)" í•„ìˆ˜)

[ê²€ì¦í•˜ì§€ ë§ì•„ì•¼ í•  ê²ƒë“¤ - ì ˆëŒ€ ë¬¸ì œë¡œ í‘œì‹œí•˜ì§€ ë§ˆì„¸ìš”!]
âŒ Standardì— ì—†ëŠ” í•¨ëŸ‰ ì •ë³´(%)ê°€ Designì— ì¶”ê°€ëœ ê²½ìš°
âŒ ë¼ë²¨ëª…ì€ ì—†ì§€ë§Œ ë‚´ìš©ì´ë‚˜ ìˆ˜ì¹˜ê°€ ìˆëŠ” ê²½ìš°
âŒ ê³µë°±ì´ë‚˜ í¬ë§·íŒ… ì°¨ì´ë§Œ ìˆëŠ” ê²½ìš° (ì˜ˆ: "íƒœêµ­, ë² íŠ¸ë‚¨ì‚°" vs "íƒœêµ­,ë² íŠ¸ë‚¨ì‚°")
âŒ Standardì™€ Designì´ ì˜ë¯¸ìƒ ë™ì¼í•˜ì§€ë§Œ í‘œí˜„ë§Œ ë‹¤ë¥¸ ê²½ìš°

[ì¤‘ìš” ê·œì¹™ - hallucination ë°©ì§€]
- "expected" ê°’ì€ ë°˜ë“œì‹œ Standard JSON í…ìŠ¤íŠ¸ì—ì„œ ì‹¤ì œë¡œ ì¡´ì¬í•˜ëŠ” ë¬¸ìì—´ì„ ê·¸ëŒ€ë¡œ ë³µì‚¬í•´ì„œ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.
- "actual" ê°’ì€ ë°˜ë“œì‹œ ë””ìì¸ OCR í…ìŠ¤íŠ¸(design_ocr_text)ì—ì„œ ì‹¤ì œë¡œ ì¡´ì¬í•˜ëŠ” ë¬¸ìì—´ì„ ê·¸ëŒ€ë¡œ ë³µì‚¬í•´ì„œ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.
- Standardë‚˜ ë””ìì¸ OCR í…ìŠ¤íŠ¸ì— ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ìˆ«ì, ë‹¨ìœ„, ë¬¸êµ¬ë¥¼ ìƒìƒí•´ì„œ ë§Œë“¤ë©´ ì•ˆ ë©ë‹ˆë‹¤.
- ì¡´ì¬í•˜ì§€ ì•ŠëŠ” 500g, ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ì˜¤íƒ€ ë“±ì„ ìƒìƒìœ¼ë¡œ ë§Œë“¤ì§€ ë§ˆì„¸ìš”.

[ê²€ì¦ í•­ëª©]
1. **ì›ì¬ë£Œëª… ì˜¤íƒˆì**
2. **ìˆ«ì/ë‹¨ìœ„ ì˜¤íƒˆì**
3. **êµ¬ë‘ì  ì˜¤íƒˆì**
4. **ì›ì‚°ì§€ ì˜¤ê¸°**
5. **ìˆœì„œ ìœ„ë°˜**
6. **ë²•ë¥  ìœ„ë°˜**
7. **ë¹„ì •ìƒ ê°’**

[ì¶œë ¥ ì–‘ì‹ - JSON]
{
    "design_ocr_text": "ë””ìì¸ íŒŒì¼ì—ì„œ ì¸ì‹í•œ í…ìŠ¤íŠ¸ (ì…ë ¥ìœ¼ë¡œ ë°›ì€ OCR í…ìŠ¤íŠ¸ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©)",
    "score": 90,
    "law_compliance": {
        "status": "compliant" | "violation",
        "violations": ["ë²•ë¥  ìœ„ë°˜ ì‚¬í•­ ëª©ë¡ - ë²•ë¥  ì¡°í•­ë§Œ í‘œì‹œ"]
    },
    "issues": [
        {
            "type": "Critical" | "Minor" | "Law_Violation",
            "location": "ìœ„ì¹˜ ì„¤ëª…",
            "issue": "ì˜¤ë¥˜ ìœ í˜•",
            "expected": "ì •ë‹µ ë‚´ìš© (Standard ê¸°ì¤€, ë°˜ë“œì‹œ Standardì—ì„œ ì‹¤ì œ ìˆëŠ” í…ìŠ¤íŠ¸)",
            "actual": "ì‹¤ì œ ë‚´ìš© (Design OCRì—ì„œ ì‹¤ì œ ìˆëŠ” í…ìŠ¤íŠ¸)",
            "suggestion": "ìˆ˜ì • ì œì•ˆ",
            "law_reference": "ê´€ë ¨ ë²•ë ¹ ì¡°í•­ (ë²•ë¥  ìœ„ë°˜ì¸ ê²½ìš°ë§Œ)"
        }
    ]
}
"""


# =======================
#  í…ìŠ¤íŠ¸/HTML ì •ë¦¬
# =======================

def clean_html_text(text):
    if not text:
        return ""
    text = html.unescape(str(text))
    prev_text = ""
    while prev_text != text:
        prev_text = text
        text = re.sub(r"<[^>]+>", "", text)
    text = re.sub(r'style\s*=\s*["\'][^"\']*["\']', "", text, flags=re.IGNORECASE)
    text = re.sub(r'class\s*=\s*["\'][^"\']*["\']', "", text, flags=re.IGNORECASE)
    text = re.sub(r"font-weight\s*:\s*\d+", "", text, flags=re.IGNORECASE)
    text = re.sub(r"margin[^;]*;?", "", text, flags=re.IGNORECASE)
    text = re.sub(r"padding[^;]*;?", "", text, flags=re.IGNORECASE)
    text = re.sub(r"color[^;]*;?", "", text, flags=re.IGNORECASE)
    text = re.sub(r"font-size[^;]*;?", "", text, flags=re.IGNORECASE)
    text = re.sub(r"\s+", " ", text)
    return text.strip()


def clean_ai_response(data):
    if isinstance(data, dict):
        cleaned = {}
        for key, value in data.items():
            if key in ["violations", "issues"] and isinstance(value, list):
                cleaned[key] = []
                for item in value:
                    if isinstance(item, dict):
                        cleaned_item = {}
                        for k, v in item.items():
                            if isinstance(v, str):
                                cleaned_item[k] = clean_html_text(v)
                            else:
                                cleaned_item[k] = clean_ai_response(v)
                        cleaned[key].append(cleaned_item)
                    elif isinstance(item, str):
                        cleaned[key].append(clean_html_text(item))
                    else:
                        cleaned[key].append(clean_ai_response(item))
            elif isinstance(value, str):
                cleaned[key] = clean_html_text(value)
            else:
                cleaned[key] = clean_ai_response(value)
        return cleaned
    elif isinstance(data, list):
        return [clean_ai_response(item) for item in data]
    elif isinstance(data, str):
        return clean_html_text(data)
    else:
        return data


# =======================
#  OCR
# =======================

def ocr_image_bytes(image_bytes: bytes) -> str:
    """
    ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ëŠ” OCR í•¨ìˆ˜
    1ìˆœìœ„: OpenAI Vision
    2ìˆœìœ„: pytesseract (ì„¤ì¹˜ë˜ì–´ ìˆëŠ” ê²½ìš°)
    """
    # 1) OpenAI Vision ê¸°ë°˜ OCR
    try:
        resized_bytes, fmt = resize_image_bytes(image_bytes, max_size=1600)
        mime_type = f"image/{fmt.lower()}"
        b64_image = base64.b64encode(resized_bytes).decode("utf-8")
        data_url = f"data:{mime_type};base64,{b64_image}"

        ocr_prompt = """
ì´ ì´ë¯¸ì§€ëŠ” ì‹í’ˆ í¬ì¥ì§€/ë¼ë²¨ ë“±ì˜ ì‚¬ì§„ì…ë‹ˆë‹¤.
ì´ë¯¸ì§€ ì•ˆì— ë³´ì´ëŠ” ëª¨ë“  ê¸€ìë¥¼ **ê·¸ëŒ€ë¡œ** ì¸ì‹í•´ì„œ ì ì–´ ì£¼ì„¸ìš”.

[ì¤‘ìš”]
- ì¤„ë°”ê¿ˆ, ê³µë°±, ìˆ«ì, ê¸°í˜¸ë¥¼ ìµœëŒ€í•œ ì›ë¬¸ ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ì„¸ìš”.
- ì˜ë¯¸ë¥¼ ìš”ì•½í•˜ê±°ë‚˜ ì„¤ëª…í•˜ì§€ ë§ê³ , ìˆœìˆ˜ í…ìŠ¤íŠ¸ë§Œ ì¶œë ¥í•˜ì„¸ìš”.
- í•œêµ­ì–´ëŠ” í•œêµ­ì–´ë¡œ, ì˜ì–´/ìˆ«ìëŠ” ìˆëŠ” ê·¸ëŒ€ë¡œ ì ì–´ ì£¼ì„¸ìš”.
"""

        input_items = [
            {
                "role": "user",
                "content": [
                    {"type": "input_text", "text": ocr_prompt.strip()},
                    {"type": "input_image", "image_url": {"url": data_url}},
                ],
            }
        ]

        resp = call_openai_response(VISION_MODEL, input_items)
        text = extract_output_text_from_response(resp).strip()

        # ì½”ë“œë¸”ë¡ ì œê±°
        if text.startswith("```"):
            lines = text.split("\n")
            if lines and lines[0].startswith("```"):
                lines = lines[1:]
            if lines and lines[-1].strip().startswith("```"):
                lines = lines[:-1]
            text = "\n".join(lines).strip()

        if text:
            print("âœ… OpenAI Vision OCR ì„±ê³µ")
            return text
        else:
            print("âš ï¸ OpenAI Vision OCR ê²°ê³¼ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print("âŒ OpenAI Vision OCR ì‹¤íŒ¨:", e)

    # 2) pytesseract í´ë°±
    if TESSERACT_AVAILABLE:
        try:
            img = PIL.Image.open(io.BytesIO(image_bytes)).convert("RGB")
            text = pytesseract.image_to_string(img, lang="kor+eng")
            text = text.strip()
            if text:
                print("âœ… pytesseract OCR ì„±ê³µ (í´ë°±)")
            else:
                print("âš ï¸ pytesseract OCR ê²°ê³¼ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
            return text
        except Exception as e:
            print("pytesseract OCR ì‹¤íŒ¨:", e)

    print("âš ï¸ OCR ê²°ê³¼ë¥¼ ì–»ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
    return ""


# =======================
#  íŒŒì¼ ì²˜ë¦¬
# =======================

def process_file_to_part(file_storage):
    """
    íŒŒì¼ì„ ëª¨ë¸ì— ì¤„ ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ ë³€í™˜
    - Excel: CSV í…ìŠ¤íŠ¸
    - ì´ë¯¸ì§€: bytes (OCRìš©)
    - ê¸°íƒ€: ê°„ë‹¨ ì„¤ëª… í…ìŠ¤íŠ¸
    """
    mime_type = file_storage.mimetype
    file_data = file_storage.read()
    file_storage.seek(0)

    if mime_type in [
        "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
        "application/vnd.ms-excel",
    ]:
        try:
            df = pd.read_excel(io.BytesIO(file_data))
            csv_text = df.to_csv(index=False)
            return {"text": f"--- [Excel ë°°í•©ë¹„ ë°ì´í„°] ---\n{csv_text}"}
        except Exception as e:
            print(f"ì—‘ì…€ ë³€í™˜ ì‹¤íŒ¨: {e}")
            return None

    if mime_type.startswith("image/"):
        return {"mime_type": mime_type, "data": file_data}

    return {
        "text": f"[íŒŒì¼] ì´ë¦„: {file_storage.filename}, MIME: {mime_type}, í¬ê¸°: {len(file_data)} bytes"
    }


def extract_ingredient_info_from_image(image_file):
    """
    ì›ì¬ë£Œ í‘œì‹œì‚¬í•­ ì´ë¯¸ì§€ì—ì„œ í•„ìš”í•œ ì •ë³´ë§Œ ì¶”ì¶œ
    1ìˆœìœ„: OpenAI Vision + JSON
    2ìˆœìœ„: ë‹¨ìˆœ OCR í…ìŠ¤íŠ¸
    """
    try:
        image_data = image_file.read()
        image_file.seek(0)

        resized_bytes, fmt = resize_image_bytes(image_data, max_size=1500)
        mime_type = image_file.mimetype or f"image/{fmt.lower()}"
        b64_image = base64.b64encode(resized_bytes).decode("utf-8")
        data_url = f"data:{mime_type};base64,{b64_image}"

        input_items = [
            {
                "role": "user",
                "content": [
                    {
                        "type": "input_text",
                        "text": PROMPT_EXTRACT_INGREDIENT_INFO.strip(),
                    },
                    {"type": "input_image", "image_url": {"url": data_url}},
                ],
            }
        ]

        resp = call_openai_response(
            VISION_MODEL,
            input_items,
            response_format={"type": "json_object"},
        )

        result_text = extract_output_text_from_response(resp).strip()
        print("---- extract_ingredient_info_from_image ì‘ë‹µ ----")
        print(result_text[:1000])

        if not result_text:
            ocr_text = ocr_image_bytes(image_data)
            if ocr_text:
                return {"ocr_fallback_text": ocr_text}
            return None

        if result_text.startswith("```json"):
            result_text = result_text[7:-3] if result_text.endswith("```") else result_text[7:]
        elif result_text.startswith("```"):
            blocks = result_text.split("```")
            if len(blocks) > 1:
                result_text = blocks[1].strip()

        return json.loads(result_text)
    except json.JSONDecodeError as e:
        print("ì›ì¬ë£Œ JSON íŒŒì‹± ì‹¤íŒ¨:", e)
        print("ì‘ë‹µ ì¼ë¶€:", result_text[:500])
        return None
    except Exception as e:
        print("ì›ì¬ë£Œ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨:", e)
        return None


# =======================
#  í•˜ì´ë¼ì´íŠ¸ HTML / ê¸°íƒ€ ìœ í‹¸
# =======================

def simple_generate_highlight_html(ocr_text: str, standard_ingredients: list[str]) -> str:
    lines = ocr_text.splitlines()
    std_lower = [s.lower() for s in standard_ingredients]
    html_lines = []
    for line in lines:
        lowered = line.lower()
        line_html = html.escape(line)
        matched = False
        for idx, std in enumerate(std_lower):
            if std in lowered:
                matched = True
                line_html = line_html.replace(
                    html.escape(standard_ingredients[idx]),
                    f"<span style='background:#e6f4ea;padding:2px 4px;border-radius:4px;'>{html.escape(standard_ingredients[idx])}</span>",
                )
        if not matched:
            line_html = (
                f"<span style='color:#ad2e2e; font-weight:600;'>{line_html}</span>"
            )
        html_lines.append(
            f"<div style='margin-bottom:6px; font-family:monospace; white-space:pre-wrap;'>{line_html}</div>"
        )
    result_html = (
        "<div style='padding:10px; background:#fff; border-radius:8px;'>"
        + "".join(html_lines)
        + "</div>"
    )
    return result_html


def extract_text_from_design_part(design_part):
    try:
        from PIL import Image
        pil_type = Image.Image
    except Exception:
        pil_type = None
    if pil_type and isinstance(design_part, pil_type):
        bio = BytesIO()
        design_part.save(bio, format="PNG")
        bio.seek(0)
        img_bytes = bio.read()
        return ocr_image_bytes(img_bytes)
    if isinstance(design_part, dict) and "data" in design_part:
        img_bytes = design_part["data"]
        return ocr_image_bytes(img_bytes)
    return ""


def filter_issues_by_text_evidence(result, standard_json: str, ocr_text: str):
    """
    LLM hallucination ë°©ì§€ í•„í„°:
    - expectedê°€ Standardì— ì‹¤ì œ ì¡´ì¬í•˜ëŠ”ì§€
    - actualì´ OCR í…ìŠ¤íŠ¸ì— ì‹¤ì œ ì¡´ì¬í•˜ëŠ”ì§€
    í™•ì¸ í›„, ë‘˜ ì¤‘ í•˜ë‚˜ë¼ë„ ì—†ìœ¼ë©´ ì´ìŠˆì—ì„œ ì œê±°
    """
    if not isinstance(result, dict):
        return result

    # Standard í…ìŠ¤íŠ¸ í¼ì¹˜ê¸°
    try:
        std_obj = json.loads(standard_json) if standard_json else {}
        std_text = json.dumps(std_obj, ensure_ascii=False)
    except Exception:
        std_text = standard_json or ""

    issues = result.get("issues", [])
    if not isinstance(issues, list):
        return result

    filtered = []
    for issue in issues:
        if not isinstance(issue, dict):
            continue
        expected = str(issue.get("expected", "") or "")
        actual = str(issue.get("actual", "") or "")

        ok_expected = (expected == "") or (expected in std_text)
        ok_actual = (actual == "") or (actual in ocr_text)

        if ok_expected and ok_actual:
            filtered.append(issue)
        else:
            print("ğŸš« hallucination ì˜ì‹¬ ì´ìŠˆ ì œê±°:", {"expected": expected, "actual": actual})

    result["issues"] = filtered
    return result


def mark_possible_ocr_error_issues(result, max_edit_distance: int = 2):
    """
    expected / actual ê°„ ë¬¸ì ì°¨ì´ê°€ ë„ˆë¬´ ì‘ìœ¼ë©´
    -> 'OCR ì˜¤ë¥˜ ê°€ëŠ¥ì„±' í”Œë˜ê·¸ë¥¼ ë‹¬ê³ , ì‹¬ê°ë„ë¥¼ í•œ ë‹¨ê³„ ë‚®ì¶˜ë‹¤.

    max_edit_distance: í—ˆìš©í•  ìµœëŒ€ í¸ì§‘ ê±°ë¦¬ (1~2 ì •ë„ ì¶”ì²œ)
    """
    if not isinstance(result, dict):
        return result

    issues = result.get("issues", [])
    if not isinstance(issues, list):
        return result

    def approx_distance(a: str, b: str) -> int:
        """Levenshtein ëŒ€ì‹  SequenceMatcherë¡œ ê·¼ì‚¬ ê±°ë¦¬ ê³„ì‚°"""
        if not a or not b:
            return 999
        s = difflib.SequenceMatcher(None, a, b)
        return int(round((1.0 - s.ratio()) * max(len(a), len(b))))

    for issue in issues:
        if not isinstance(issue, dict):
            continue
        expected = str(issue.get("expected", "") or "").strip()
        actual = str(issue.get("actual", "") or "").strip()

        if not expected or not actual:
            continue

        dist = approx_distance(expected, actual)
        min_len = min(len(expected), len(actual))

        # ê¸€ì ê¸¸ì´ê°€ ë„ˆë¬´ ì§§ìœ¼ë©´ ë…¸ì´ì¦ˆë¼ì„œ ì œì™¸, ìµœì†Œ 3ì ì´ìƒë§Œ íŒë‹¨
        if min_len >= 3 and dist <= max_edit_distance:
            # OCR ì˜¤ë¥˜ ê°€ëŠ¥ì„± ë†’ìŒ
            flags = issue.setdefault("flags", [])
            if "possible_ocr_error" not in flags:
                flags.append("possible_ocr_error")

            # ì‹¬ê°ë„ ì¡°ì •: Law_Violation â†’ Minor
            old_type = issue.get("type", "")
            if old_type == "Law_Violation":
                issue["type"] = "Minor"

            # ì„¤ëª…ì— í•œ ì¤„ ì¶”ê°€
            desc = issue.get("issue", "")
            if "OCR ì˜¤ë¥˜ ê°€ëŠ¥ì„±" not in desc:
                issue["issue"] = (desc + " (OCR ì˜¤ë¥˜ ê°€ëŠ¥ì„± ìˆìŒ)").strip()

            print("ğŸŸ¡ OCR ì˜ì‹¬ ì´ìŠˆ:", {
                "expected": expected,
                "actual": actual,
                "distance": dist
            })

    return result


# =======================
#  ë¼ìš°íŠ¸
# =======================

@app.route("/")
def index():
    return render_template("index.html")


# ---- ë””ìì¸ ê²€ì¦ ----

@app.route("/api/verify-design", methods=["POST"])
def verify_design():
    print("ğŸ•µï¸â€â™‚ï¸ 2ë‹¨ê³„: ë””ìì¸ ê²€ì¦ ì‹œì‘ (OCR + hallucination í•„í„°)...")
    design_file = request.files.get("design_file")
    standard_excel = request.files.get("standard_excel")
    standard_json = request.form.get("standard_data")

    if not design_file:
        return jsonify({"error": "ë””ìì¸ íŒŒì¼ì´ í•„ìš”í•©ë‹ˆë‹¤."}), 400
    if not standard_excel and not standard_json:
        return jsonify({"error": "ê¸°ì¤€ ë°ì´í„°(ì—‘ì…€ íŒŒì¼ ë˜ëŠ” JSON)ê°€ í•„ìš”í•©ë‹ˆë‹¤."}), 400

    # 1) ê¸°ì¤€ ë°ì´í„° Excel â†’ JSON ë³€í™˜ (ì˜µì…˜)
    if standard_excel:
        try:
            df_dict = pd.read_excel(
                io.BytesIO(standard_excel.read()),
                sheet_name=None,
                engine="openpyxl",
            )
            if not df_dict:
                return jsonify({"error": "ì—‘ì…€ íŒŒì¼ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤."}), 400
            first_sheet_name = list(df_dict.keys())[0]
            first_sheet_df = df_dict[first_sheet_name]
            if not first_sheet_df.empty:
                first_column = first_sheet_df.columns[0]
                if "ì›ì¬ë£Œëª…" in first_sheet_df.columns:
                    ingredients_list = (
                        first_sheet_df["ì›ì¬ë£Œëª…"].dropna().astype(str).tolist()
                    )
                else:
                    ingredients_list = (
                        first_sheet_df[first_column].dropna().astype(str).tolist()
                    )
                standard_data = {
                    "ingredients": {
                        "structured_list": ingredients_list,
                        "continuous_text": ", ".join(ingredients_list),
                    }
                }
                standard_json = json.dumps(standard_data, ensure_ascii=False)
            else:
                return jsonify({"error": "ì—‘ì…€ì˜ ì²« ì‹œíŠ¸ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤."}), 400
        except Exception as e:
            print(f"âŒ ì—‘ì…€ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {e}")
            traceback.print_exc()
            return jsonify({"error": f"ì—‘ì…€ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {str(e)}"}), 400

    # 2) ë””ìì¸ íŒŒì¼ OCR ìˆ˜í–‰
    design_part = process_file_to_part(design_file)
    ocr_text = ""
    try:
        ocr_text = extract_text_from_design_part(design_part)
        if not ocr_text:
            raw_bytes = design_file.read()
            design_file.seek(0)
            ocr_text = ocr_image_bytes(raw_bytes)
    except Exception as e:
        print("ë””ìì¸ OCR ì‹¤íŒ¨:", e)
        traceback.print_exc()
        ocr_text = ""

    print("===== DESIGN OCR TEXT (first 1000 chars) =====")
    print((ocr_text or "")[:1000])
    print("==============================================")

    # 3) ChatGPTì— ê²€ì¦ ìš”ì²­ (í…ìŠ¤íŠ¸ ê¸°ë°˜)
    parts = []
    enhanced_prompt = PROMPT_VERIFY_DESIGN
    if ALL_LAW_TEXT:
        enhanced_prompt += (
            f"\n\n--- [ì°¸ê³  ë²•ë ¹] ---\n{ALL_LAW_TEXT}\n--- [ë²•ë ¹ ë] ---\n"
        )
    parts.append(enhanced_prompt)
    parts.append(f"\n--- [ê¸°ì¤€ ë°ì´í„°(Standard)] ---\n{standard_json}\n")
    parts.append(
        f"\n--- [ë””ìì¸ OCR í…ìŠ¤íŠ¸] ---\n{ocr_text}\n--- [ë””ìì¸ OCR í…ìŠ¤íŠ¸ ë] ---\n"
    )

    prompt_text = combine_parts_to_prompt(parts)

    result_text = ""
    result = None

    try:
        resp = call_openai_response(
            TEXT_MODEL,
            prompt_text,
            response_format={"type": "json_object"},
        )
        result_text = extract_output_text_from_response(resp).strip()
        print("---- ëª¨ë¸ ì‘ë‹µ(ì›ë¬¸) ì‹œì‘ ----")
        print(result_text[:4000])
        print("---- ëª¨ë¸ ì‘ë‹µ(ì›ë¬¸) ë ----")
    except Exception as e:
        print("ëª¨ë¸ í˜¸ì¶œ ì‹¤íŒ¨:", e)
        traceback.print_exc()
        result_text = ""

    if result_text:
        if result_text.startswith("```json"):
            result_text = result_text[7:]
            if result_text.endswith("```"):
                result_text = result_text[:-3]
        elif result_text.startswith("```"):
            lines = result_text.split("\n")
            if lines and lines[0].startswith("```"):
                result_text = "\n".join(lines[1:])
            if result_text.endswith("```"):
                result_text = result_text[:-3]
        result_text = result_text.strip()
        try:
            result = json.loads(result_text)
        except json.JSONDecodeError as json_err:
            print("JSON íŒŒì‹± ì˜¤ë¥˜:", json_err)
            print("ì‘ë‹µ í…ìŠ¤íŠ¸(ì¼ë¶€):", result_text[:1000])
            try:
                fixed = (
                    result_text.replace(",\n}", "\n}")
                    .replace(",\n]", "\n]")
                    .replace(", }", " }")
                    .replace(", ]", " ]")
                )
                result = json.loads(fixed)
                print("âœ… JSON ìˆ˜ì • í›„ íŒŒì‹± ì„±ê³µ")
            except Exception as e:
                print("ìµœì¢… JSON íŒŒì‹± ì‹¤íŒ¨:", e)
                result = None

    # 4) ëª¨ë¸ì´ í•˜ì´ë¼ì´íŠ¸ ì•ˆ ì£¼ë©´ ì„œë²„ì—ì„œ ìƒì„±
    highlight_html = None
    if result and isinstance(result, dict):
        highlight_html = result.get("design_ocr_highlighted_html") or None

    if not highlight_html:
        print("ëª¨ë¸ì—ì„œ í•˜ì´ë¼ì´íŠ¸ë¥¼ ì œê³µí•˜ì§€ ì•ŠìŒ -> ì„œë²„ í´ë°± í•˜ì´ë¼ì´íŠ¸ ìƒì„±")
        try:
            std_ingredients = []
            try:
                std_obj = json.loads(standard_json)
                std_ingredients = std_obj.get("ingredients", {}).get(
                    "structured_list", []
                )
            except Exception:
                std_ingredients = []
            highlight_html = simple_generate_highlight_html(ocr_text or "", std_ingredients)
            if not result:
                result = {}
            result["design_ocr_highlighted_html"] = highlight_html
            result.setdefault("design_ocr_text", ocr_text)
        except Exception as e:
            print("í´ë°± OCR ì²˜ë¦¬ ì‹¤íŒ¨:", e)
            traceback.print_exc()
            if not result:
                result = {}
            result[
                "design_ocr_highlighted_html"
            ] = "<div>ì„œë²„ í´ë°± OCR ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.</div>"
            result["design_ocr_text"] = ocr_text or ""

    if not result:
        result = {
            "design_ocr_text": ocr_text,
            "score": 0,
            "law_compliance": {"status": "needs_review", "violations": []},
            "issues": [],
            "design_ocr_highlighted_html": "<div>ëª¨ë¸ê³¼ í´ë°± ëª¨ë‘ì—ì„œ OCR ê²°ê³¼ë¥¼ ì–»ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.</div>",
        }

    # 5) hallucination í•„í„° ì ìš© (expected/actualì´ ì‹¤ì œ í…ìŠ¤íŠ¸ì— ìˆëŠ”ì§€ ê²€ì¦)
    result = filter_issues_by_text_evidence(result, standard_json or "", ocr_text or "")

    # 6) OCR ì˜ì‹¬ ì´ìŠˆ í‘œì‹œ (expected/actual ì°¨ì´ê°€ ë§¤ìš° ì‘ì€ ê²½ìš°)
    result = mark_possible_ocr_error_issues(result, max_edit_distance=2)

    # 7) HTML íƒœê·¸ ì •ë¦¬
    result = clean_ai_response(result)

    return jsonify(result)


# ---- QA ìë£Œ ì—…ë¡œë“œ & í‘œì‹œì‚¬í•­ ì‘ì„± ----

@app.route("/api/upload-qa", methods=["POST"])
def upload_qa():
    print("ğŸ“‹ QA ìë£Œ ì—…ë¡œë“œ ë° ì‹í’ˆí‘œì‹œì‚¬í•­ ì‘ì„± ì‹œì‘...")
    qa_files = request.files.getlist("qa_files")
    if not qa_files or len(qa_files) == 0:
        return jsonify({"error": "QA ìë£Œ íŒŒì¼ì´ í•„ìš”í•©ë‹ˆë‹¤."}), 400

    parts = []
    qa_prompt = """
ë‹¹ì‹ ì€ ì‹í’ˆí‘œì‹œì‚¬í•­ ì‘ì„± ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì œê³µëœ QA ìë£Œë¥¼ ë¶„ì„í•˜ì—¬ ë²•ë¥ ì„ ì¤€ìˆ˜í•˜ëŠ” ì‹í’ˆí‘œì‹œì‚¬í•­ì„ ì‘ì„±í•˜ì„¸ìš”.

[ì‘ì—… ë‹¨ê³„]
1. QA ìë£Œ ë¶„ì„: ì—‘ì…€, ì´ë¯¸ì§€ ë“± ëª¨ë“  QA ìë£Œë¥¼ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•˜ì„¸ìš”.
2. ë²•ë¥  ê²€í† : ì œê³µëœ ë²•ë ¹ì„ ì°¸ê³ í•˜ì—¬ í•„ìˆ˜ í‘œì‹œì‚¬í•­ì´ ëª¨ë‘ í¬í•¨ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.
3. ì‹í’ˆí‘œì‹œì‚¬í•­ ì‘ì„±: ë²•ë¥ ì„ ì¤€ìˆ˜í•˜ëŠ” ì™„ì „í•œ ì‹í’ˆí‘œì‹œì‚¬í•­ì„ ì‘ì„±í•˜ì„¸ìš”.

[ì¶œë ¥ ì–‘ì‹ - JSON]
{
    "product_name": "ì œí’ˆëª…",
    "label_text": "ì‘ì„±ëœ ì‹í’ˆí‘œì‹œì‚¬í•­ ì „ì²´ í…ìŠ¤íŠ¸",
    "law_compliance": {
        "status": "compliant" | "needs_review",
        "issues": ["ë²•ë¥  ê²€í†  ì‚¬í•­ ëª©ë¡"]
    },
    "sections": {
        "ingredients": "ì›ì¬ë£Œëª…",
        "nutrition": "ì˜ì–‘ì •ë³´",
        "allergens": "ì•Œë ˆë¥´ê¸° ìœ ë°œë¬¼ì§ˆ",
        "storage": "ë³´ê´€ë°©ë²•",
        "manufacturer": "ì œì¡°ì‚¬ ì •ë³´"
    }
}
"""
    if ALL_LAW_TEXT:
        qa_prompt += (
            f"\n\n--- [ì°¸ê³  ë²•ë ¹] ---\n{ALL_LAW_TEXT}\n--- [ë²•ë ¹ ë] ---\n"
        )
    parts.append(qa_prompt)

    for qa_file in qa_files[:20]:
        file_part = process_file_to_part(qa_file)
        if not file_part:
            continue
        if isinstance(file_part, dict) and "text" in file_part:
            parts.append(file_part["text"])
        else:
            parts.append(str(file_part))

    print(f"ğŸ“‚ QA ìë£Œ ì²˜ë¦¬ ì¤‘: {len(qa_files)}ê°œ íŒŒì¼")

    try:
        prompt_text = combine_parts_to_prompt(parts)
        resp = call_openai_response(
            TEXT_MODEL,
            prompt_text,
            response_format={"type": "json_object"},
        )

        result_text = extract_output_text_from_response(resp).strip()

        print("---- QA ëª¨ë¸ ì‘ë‹µ(ì›ë¬¸) ì‹œì‘ ----")
        print(result_text[:4000])
        print("---- QA ëª¨ë¸ ì‘ë‹µ(ì›ë¬¸) ë ----")

        if result_text.startswith("```json"):
            result_text = result_text[7:]
            if result_text.endswith("```"):
                result_text = result_text[:-3]
        elif result_text.startswith("```"):
            lines = result_text.split("\n")
            if lines and lines[0].startswith("```"):
                result_text = "\n".join(lines[1:])
            if result_text.endswith("```"):
                result_text = result_text[:-3]
        result_text = result_text.strip()

        try:
            result = json.loads(result_text)
        except json.JSONDecodeError as json_err:
            print(f"âŒ JSON íŒŒì‹± ì˜¤ë¥˜: {json_err}")
            print(f"ì‘ë‹µ í…ìŠ¤íŠ¸ (ì²˜ìŒ 2000ì): {result_text[:2000]}")
            try:
                result_text_fixed = (
                    result_text.replace(",\n}", "\n}")
                    .replace(",\n]", "\n]")
                    .replace(", }", " }")
                    .replace(", ]", " ]")
                )
                result = json.loads(result_text_fixed)
                print("âœ… JSON ìˆ˜ì • í›„ íŒŒì‹± ì„±ê³µ")
            except Exception:
                return jsonify(
                    {
                        "error": f"JSON íŒŒì‹± ì‹¤íŒ¨: {str(json_err)}. ì‘ë‹µì˜ ì¼ë¶€: {result_text[:200]}..."
                    }
                ), 500

        result = clean_ai_response(result)
        return jsonify(result)

    except Exception as e:
        print("âŒ QA ìë£Œ ì²˜ë¦¬ ì˜¤ë¥˜:", e)
        traceback.print_exc()
        return jsonify({"error": str(e)}), 500


# =======================
#  ë©”ì¸
# =======================

if __name__ == "__main__":
    print("ğŸš€ ì‚¼ì§„ì–´ë¬µ ì‹í’ˆí‘œì‹œì‚¬í•­ ì™„ì„± í”Œë«í¼ V3.0 (OpenAI ë²„ì „) ê°€ë™")
    from waitress import serve

    serve(
        app,
        host="0.0.0.0",
        port=8080,
        threads=4,
        channel_timeout=600,
    )
